# kitti_infer.py
import os
import cv2
import json
import time
import argparse
import numpy as np
from tqdm import tqdm
from src.detector import PersonDetector
from src.geolocalizer import GeoLocalizer
from src.visualizer import Visualizer

# KITTI è½¦è½½ç›¸æœºé«˜åº¦ (å›ºå®šå€¼)
KITTI_CAM_HEIGHT = 1.65 

def parse_calib(calib_path):
    """ä» P2 çŸ©é˜µæå–ç„¦è· fx"""
    if not os.path.exists(calib_path): return None
    with open(calib_path, 'r') as f:
        for line in f.readlines():
            if line.startswith('P2:'):
                # P2 æ˜¯ 3x4 çŸ©é˜µï¼ŒP2[0,0] å³ä¸º fx (åƒç´ ç„¦è·)
                return float(line.split()[1])
    return None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--kitti_root', type=str, default='data/kitti', help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='data/kitti_rsts', help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--mode', type=str, default='mount', choices=['flat', 'mount'], help='æµ‹è·æ¨¡å¼')
    parser.add_argument('--limit', type=int, default=0, help='æµ‹è¯•æ•°é‡é™åˆ¶')
    args = parser.parse_args()

    # 1. ç›®å½•å‡†å¤‡
    base_img_dir = os.path.join(args.kitti_root, 'data_object_image_2', 'training', 'image_2')
    base_calib_dir = os.path.join(args.kitti_root, 'data_object_calib', 'training', 'calib')
    
    save_json_dir = os.path.join(args.output_dir, 'data')
    save_vis_dir = os.path.join(args.output_dir, 'vis')
    os.makedirs(save_json_dir, exist_ok=True)
    os.makedirs(save_vis_dir, exist_ok=True)

    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("Loading models...")
    # ç¡®ä¿ä½ çš„ detector.py å·²æ›´æ–°æ”¯æŒ print æ—¶é—´
    detector = PersonDetector('./models/Detect/yolo26l.pt', './models/Pose/yolo11x-pose.pt')
    visualizer = Visualizer()

    # ================= [æ–°å¢] 1. GPU å†·å¯åŠ¨é¢„çƒ­ (Warmup) =================
    print("ğŸ”¥ Warming up GPU...")
    dummy_img = np.zeros((375, 1242, 3), dtype=np.uint8)
    # å¼ºåˆ¶è¿è¡Œä¸€æ¬¡ Detect å’Œ Pose
    detector.detect(dummy_img, use_pose=False) 
    if detector.pose_model:
        detector.pose_model(dummy_img, verbose=False)
    print("âœ… Warmup complete. Starting benchmark...")
    # ====================================================================

    img_files = sorted([f for f in os.listdir(base_img_dir) if f.endswith('.png')])
    if args.limit > 0: img_files = img_files[:args.limit]

    print(f"Start Inference on {len(img_files)} images...")
    
    # 3. æ‰¹é‡æ¨ç†
    for img_file in tqdm(img_files):
        file_id = os.path.splitext(img_file)[0]
        
        # A. è¯»å›¾
        img_path = os.path.join(base_img_dir, img_file)
        frame = cv2.imread(img_path)
        h, w = frame.shape[:2]

        # B. è¯» Calib è·å–ç„¦è·
        calib_path = os.path.join(base_calib_dir, f"{file_id}.txt")
        fx = parse_calib(calib_path)
        if fx is None: continue

        # C. åŠ¨æ€é…ç½® Geolocalizer
        # æŠ€å·§: è®¾ sensor_width_mm = w, focal_length_mm = fx
        # è¿™æ ·å†…éƒ¨è®¡ç®—: f_pix = fx * (w/w) = fxï¼Œå®Œç¾å¯¹é½
        config = {
            'gps': {'lat': 0, 'lng': 0, 'alt': 0},
            'height': KITTI_CAM_HEIGHT,
            'pose': {'pitch': 0, 'yaw': 0, 'roll': 0}, # è½¦è½½ç›¸æœº pitch è¿‘ä¼¼ 0
            'hardware': {'focal_length_mm': fx, 'sensor_width_mm': w}
        }
        localizer = GeoLocalizer(config)

        # D. æ¨ç† (è®¡æ—¶)
        t_start_total = time.time()
        
        # A. Detection é˜¶æ®µ
        t_det_start = time.time()
        detections = detector.detect(frame, use_pose=False) # åªè·‘æ£€æµ‹
        t_det_end = time.time()
        
        # B. Pose é˜¶æ®µ (æ‰‹åŠ¨å¤ç° detector å†…éƒ¨é€»è¾‘ä»¥å®ç°ç‹¬ç«‹è®¡æ—¶)
        t_pose_start = time.time()
        if args.mode == 'mount':
            for det in detections:
                bbox = det['bbox']
                x1, y1, x2, y2 = bbox
                
                # 1. Padding & Crop (é€»è¾‘éœ€ä¸ Detector ä¿æŒä¸€è‡´)
                w_box, h_box = x2 - x1, y2 - y1
                pad_w, pad_h = int(w_box * 0.15), int(h_box * 0.15)
                crop_x1 = max(0, x1 - pad_w)
                crop_y1 = max(0, y1 - pad_h)
                crop_x2 = min(w, x2 + pad_w)
                crop_y2 = min(h, y2 + pad_h)
                
                person_crop = frame[crop_y1:crop_y2, crop_x1:crop_x2]
                
                # 2. Pose Inference
                if person_crop.size > 0:
                    pose_res = detector.pose_model(person_crop, verbose=False, conf=0.5)
                    
                    # 3. Coordinate Mapping
                    if (len(pose_res) > 0 and 
                        pose_res[0].keypoints is not None and 
                        pose_res[0].keypoints.data.shape[1] > 0):
                        
                        kpts_local = pose_res[0].keypoints.data[0].cpu().numpy()
                        kpts_global = []
                        for kp in kpts_local:
                            gx = kp[0] + crop_x1
                            gy = kp[1] + crop_y1
                            v = kp[2]
                            kpts_global.append([gx, gy, v])
                        
                        det['keypoints'] = kpts_global # æ³¨å…¥å› det å­—å…¸
        
        t_pose_end = time.time()

        # C. Localization é˜¶æ®µ
        processed_results = []
        for det in detections:
            # ... (è°ƒç”¨ localizer çš„é€»è¾‘ä¿æŒä¸å˜) ...
            # ... (æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ç”¨ det['keypoints'] å³å¯) ...
            
            # ä¸ºäº†å®Œæ•´æ€§å±•ç¤ºè¿™éƒ¨åˆ†ä¿®æ”¹ï¼š
            loc_res = None
            kpts = det.get('keypoints')
            if args.mode == 'mount':
                loc_res = localizer.pixel_to_location_mount(0, det['conf'], det['bbox'], (h,w), kpts)
            else:
                loc_res = localizer.pixel_to_location_flat(0, det['conf'], det['bbox'], (h,w))
            
            if loc_res:
                loc_res['target_id'] = f"P"
                loc_res['conf'] = det['conf']
                if kpts: loc_res['keypoints'] = kpts
                processed_results.append(loc_res)

        t_end_total = time.time()

        # E. ä¿å­˜ç»“æœ (JSON)
        # æ„é€ è¦ä¿å­˜çš„æ•°æ®ç»“æ„
        save_data = {
            'file_id': file_id,
            'image_size': [w, h],
            'time_stats': {
                'total_ms': (t_end_total - t_start_total) * 1000,
                'det_ms': (t_det_end - t_det_start) * 1000,
                'pose_ms': (t_pose_end - t_pose_start) * 1000, # çº¯ Pose æ¨ç†è€—æ—¶
                'post_ms': (t_end_total - t_pose_end) * 1000   # æµ‹è·ç®—æ³•è€—æ—¶
            },
            'objects': []
        }

        for res in processed_results:
            # åªä¿å­˜å¿…è¦çš„è¯„æµ‹å­—æ®µ
            obj_data = {
                'bbox': res['bbox'], # [x1, y1, x2, y2]
                'depth_pred': res['distance'],
                'conf': res['conf'],
                'mode': res.get('mode', 'N/A')
            }
            save_data['objects'].append(obj_data)

        with open(os.path.join(save_json_dir, f"{file_id}.json"), 'w') as f:
            json.dump(save_data, f, indent=2)

        # F. ä¿å­˜å¯è§†åŒ– (Mountæ¨¡å¼ä¿å­˜ Skeleton å›¾)
        if args.mode == 'mount':
            vis_img = visualizer.draw_skeleton(frame, processed_results)
        else:
            vis_img = visualizer.draw_detections(frame, processed_results)
            
        cv2.imwrite(os.path.join(save_vis_dir, f"{file_id}.jpg"), vis_img)

    print(f"\nInference Complete! Results saved to {args.output_dir}")

if __name__ == "__main__":
    main()
